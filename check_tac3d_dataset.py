#!/usr/bin/env python3
"""
æ£€æŸ¥æ•°æ®é›†ä¸­ä¿å­˜çš„Tac3Dæ•°æ®
åˆ†ææ•°æ®ç»“æ„ã€æ ¼å¼å’Œå†…å®¹
"""

import sys
import torch
import numpy as np
from pathlib import Path
from typing import Dict, Any

# Add the project root to Python path
sys.path.append(str(Path(__file__).parent))

from lerobot.common.datasets.lerobot_dataset import LeRobotDataset

def analyze_tac3d_keys(batch: Dict[str, Any]) -> Dict[str, list]:
    """åˆ†ææ‰¹æ¬¡ä¸­çš„Tac3Dç›¸å…³é”®"""
    tac3d_keys = {
        'new_format': [],      # observation.tactile.tac3d.{name}.{field}
        'old_format': [],      # observation.tactile.{name}.{field}
        'other_tactile': []    # å…¶ä»–è§¦è§‰ç›¸å…³é”®
    }
    
    for key in batch.keys():
        if key.startswith("observation.tactile."):
            parts = key.split(".")
            
            if len(parts) >= 5 and parts[2] == "tac3d":
                # æ–°æ ¼å¼ï¼šobservation.tactile.tac3d.{name}.{field}
                tac3d_keys['new_format'].append(key)
            elif len(parts) >= 4 and parts[2] != "gelsight":
                # æ—§æ ¼å¼ï¼šobservation.tactile.{name}.{field}
                tac3d_keys['old_format'].append(key)
            else:
                # å…¶ä»–è§¦è§‰æ•°æ®
                tac3d_keys['other_tactile'].append(key)
    
    return tac3d_keys

def print_tensor_info(name: str, data: torch.Tensor, level: int = 0):
    """æ‰“å°å¼ é‡çš„è¯¦ç»†ä¿¡æ¯"""
    indent = "  " * level
    print(f"{indent}{name}:")
    print(f"{indent}  - ç±»å‹: {type(data)}")
    print(f"{indent}  - å½¢çŠ¶: {data.shape}")
    print(f"{indent}  - æ•°æ®ç±»å‹: {data.dtype}")
    
    if data.numel() > 0:
        if data.dtype in [torch.float32, torch.float64]:
            print(f"{indent}  - æ•°å€¼èŒƒå›´: [{data.min().item():.6f}, {data.max().item():.6f}]")
            print(f"{indent}  - å¹³å‡å€¼: {data.mean().item():.6f}")
            print(f"{indent}  - æ ‡å‡†å·®: {data.std().item():.6f}")
            
            # æ£€æŸ¥æ˜¯å¦å…¨ä¸ºé›¶
            if torch.all(data == 0):
                print(f"{indent}  - âš ï¸  æ‰€æœ‰å€¼éƒ½ä¸ºé›¶ï¼")
            elif torch.sum(data != 0).item() < data.numel() * 0.1:
                non_zero_count = torch.sum(data != 0).item()
                print(f"{indent}  - âš ï¸  å¤§éƒ¨åˆ†å€¼ä¸ºé›¶ (éé›¶: {non_zero_count}/{data.numel()})")
        
        # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®æ ·æœ¬
        if data.numel() <= 10:
            print(f"{indent}  - æ•°æ®: {data.flatten()}")
        else:
            print(f"{indent}  - æ ·æœ¬: {data.flatten()[:5]}...")
    else:
        print(f"{indent}  - âš ï¸  å¼ é‡ä¸ºç©º")

def analyze_tac3d_data_structure(dataset_path: str):
    """åˆ†æTac3Dæ•°æ®é›†çš„æ•°æ®ç»“æ„"""
    print("ğŸ” å¼€å§‹æ£€æŸ¥Tac3Dæ•°æ®é›†...")
    print(f"æ•°æ®é›†è·¯å¾„: {dataset_path}")
    print("=" * 80)
    
    try:
        # åŠ è½½æ•°æ®é›†
        dataset = LeRobotDataset(dataset_path)
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªæ ·æœ¬")
        print(f"Episodeæ•°é‡: {dataset.num_episodes}")
        print()
        
        # æ£€æŸ¥æ•°æ®é›†å…ƒæ•°æ®
        print("ğŸ“Š æ•°æ®é›†å…ƒæ•°æ®:")
        print(f"  FPS: {dataset.fps}")
        print(f"  æ€»å¸§æ•°: {dataset.num_frames}")
        print(f"  ç›¸æœºé”®: {dataset.meta.camera_keys}")
        print(f"  æ‰€æœ‰é”®: {len(dataset.meta.features)} ä¸ª")
        print()
        
        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œåˆ†æ
        print("ğŸ¯ åˆ†æç¬¬ä¸€ä¸ªæ ·æœ¬...")
        sample = dataset[0]
        
        # åˆ†æTac3Dç›¸å…³é”®
        tac3d_keys = analyze_tac3d_keys(sample)
        
        print("ğŸ“‹ è§¦è§‰æ•°æ®é”®åˆ†ç±»:")
        print(f"  æ–°æ ¼å¼é”® (observation.tactile.tac3d.*): {len(tac3d_keys['new_format'])}")
        for key in sorted(tac3d_keys['new_format']):
            print(f"    - {key}")
        
        print(f"  æ—§æ ¼å¼é”® (observation.tactile.*): {len(tac3d_keys['old_format'])}")
        for key in sorted(tac3d_keys['old_format']):
            print(f"    - {key}")
            
        print(f"  å…¶ä»–è§¦è§‰é”®: {len(tac3d_keys['other_tactile'])}")
        for key in sorted(tac3d_keys['other_tactile']):
            print(f"    - {key}")
        print()
        
        # è¯¦ç»†åˆ†æTac3Dæ•°æ®
        all_tac3d_keys = tac3d_keys['new_format'] + tac3d_keys['old_format']
        
        if all_tac3d_keys:
            print("ğŸ“ˆ Tac3Dæ•°æ®è¯¦ç»†åˆ†æ:")
            print("-" * 60)
            
            # æŒ‰ä¼ æ„Ÿå™¨åç§°åˆ†ç»„
            sensor_data = {}
            for key in all_tac3d_keys:
                parts = key.split(".")
                if len(parts) >= 5 and parts[2] == "tac3d":
                    # æ–°æ ¼å¼
                    sensor_name = parts[3]
                    field_name = parts[4]
                elif len(parts) >= 4:
                    # æ—§æ ¼å¼
                    sensor_name = parts[2]
                    field_name = parts[3]
                else:
                    continue
                    
                if sensor_name not in sensor_data:
                    sensor_data[sensor_name] = {}
                sensor_data[sensor_name][field_name] = key
            
            # åˆ†ææ¯ä¸ªä¼ æ„Ÿå™¨çš„æ•°æ®
            for sensor_name, fields in sensor_data.items():
                print(f"\nğŸ¤– ä¼ æ„Ÿå™¨: {sensor_name}")
                print(f"  å­—æ®µæ•°é‡: {len(fields)}")
                
                for field_name, key in sorted(fields.items()):
                    data = sample[key]
                    print(f"\n  ğŸ“Š {field_name} ({key}):")
                    print_tensor_info(field_name, data, level=2)
        
        else:
            print("âŒ æœªæ‰¾åˆ°Tac3Dæ•°æ®ï¼")
            print("å¯èƒ½çš„åŸå› :")
            print("  1. æ•°æ®é›†ä¸­æ²¡æœ‰è§¦è§‰ä¼ æ„Ÿå™¨æ•°æ®")
            print("  2. è§¦è§‰æ•°æ®ä½¿ç”¨äº†ä¸åŒçš„é”®åæ ¼å¼")
            print("  3. æ•°æ®é›†æŸåæˆ–æ ¼å¼ä¸æ­£ç¡®")
            
            # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„é”®ä»¥ä¾¿è°ƒè¯•
            print(f"\nğŸ”§ æ‰€æœ‰å¯ç”¨é”® (å‰50ä¸ª):")
            all_keys = sorted(sample.keys())
            for i, key in enumerate(all_keys[:50]):
                print(f"  {i+1:2d}. {key}")
            if len(all_keys) > 50:
                print(f"  ... è¿˜æœ‰ {len(all_keys) - 50} ä¸ªé”®")
        
        # æ£€æŸ¥å¤šä¸ªæ ·æœ¬ä»¥éªŒè¯æ•°æ®ä¸€è‡´æ€§
        if len(dataset) > 1:
            print(f"\nğŸ”„ æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ (æ£€æŸ¥å‰5ä¸ªæ ·æœ¬)...")
            num_samples_to_check = min(5, len(dataset))
            
            for i in range(1, num_samples_to_check):
                sample_i = dataset[i]
                tac3d_keys_i = analyze_tac3d_keys(sample_i)
                
                if tac3d_keys_i['new_format'] != tac3d_keys['new_format'] or \
                   tac3d_keys_i['old_format'] != tac3d_keys['old_format']:
                    print(f"  âš ï¸  æ ·æœ¬ {i} çš„é”®ç»“æ„ä¸æ ·æœ¬ 0 ä¸åŒ")
                else:
                    print(f"  âœ… æ ·æœ¬ {i} é”®ç»“æ„ä¸€è‡´")
                    
                    # æ£€æŸ¥æ•°æ®å€¼
                    for key in all_tac3d_keys[:3]:  # åªæ£€æŸ¥å‰å‡ ä¸ªé”®
                        if key in sample_i:
                            data_0 = sample[key]
                            data_i = sample_i[key]
                            
                            if data_0.shape != data_i.shape:
                                print(f"    âš ï¸  {key} å½¢çŠ¶ä¸ä¸€è‡´: {data_0.shape} vs {data_i.shape}")
                            elif torch.allclose(data_0, data_i, atol=1e-6):
                                print(f"    âš ï¸  {key} æ•°å€¼å®Œå…¨ç›¸åŒ (å¯èƒ½æœªæ›´æ–°)")
        
        print(f"\nâœ… æ•°æ®é›†æ£€æŸ¥å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥æ•°æ®é›†æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        
        print(f"\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        print(f"  1. æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®: {dataset_path}")
        print(f"  2. ç¡®è®¤æ•°æ®é›†æ˜¯LeRobotæ ¼å¼")
        print(f"  3. æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å®Œæ•´")

def check_episode_data(dataset_path: str, episode_index: int = 0):
    """æ£€æŸ¥ç‰¹å®šepisodeçš„æ•°æ®"""
    print(f"\nğŸ¬ æ£€æŸ¥Episode {episode_index}çš„æ•°æ®...")
    
    try:
        dataset = LeRobotDataset(dataset_path)
        
        if episode_index >= dataset.num_episodes:
            print(f"âŒ Episode {episode_index} ä¸å­˜åœ¨ (æ€»å…± {dataset.num_episodes} ä¸ªepisodes)")
            return
        
        # è·å–episodeçš„å¸§èŒƒå›´
        from_idx = int(dataset.episode_data_index["from"][episode_index].item())
        to_idx = int(dataset.episode_data_index["to"][episode_index].item())
        num_frames = to_idx - from_idx
        
        print(f"Episode {episode_index}:")
        print(f"  å¸§èŒƒå›´: {from_idx} - {to_idx-1}")
        print(f"  å¸§æ•°é‡: {num_frames}")
        
        # æ£€æŸ¥å‰å‡ å¸§çš„Tac3Dæ•°æ®å˜åŒ–
        print(f"\nğŸ“Š å‰5å¸§çš„Tac3Dæ•°æ®å˜åŒ–:")
        
        for frame_idx in range(min(5, num_frames)):
            global_idx = from_idx + frame_idx
            sample = dataset[global_idx]
            
            print(f"\n  å¸§ {frame_idx} (å…¨å±€ç´¢å¼• {global_idx}):")
            
            # æŸ¥æ‰¾Tac3D forceæ•°æ®
            force_keys = [k for k in sample.keys() if 'resultant_force' in k and 'tactile' in k]
            
            for key in force_keys:
                force_data = sample[key]
                if isinstance(force_data, torch.Tensor) and force_data.numel() > 0:
                    magnitude = torch.norm(force_data).item()
                    print(f"    {key}: {force_data.numpy()} (å¤§å°: {magnitude:.6f})")
                else:
                    print(f"    {key}: æ— æ•°æ®")
    
    except Exception as e:
        print(f"âŒ æ£€æŸ¥episodeæ•°æ®æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import os
    
    # ä½¿ç”¨å®Œæ•´è·¯å¾„
    dataset_path = "/home/user/.cache/huggingface/lerobot/user/test_two2"
    
    print(f"æ£€æŸ¥æ•°æ®é›†è·¯å¾„: {dataset_path}")
    
    if not Path(dataset_path).exists():
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        print("è¯·ç¡®è®¤æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®")
        exit(1)
    
    # åˆ†ææ•°æ®é›†ç»“æ„
    analyze_tac3d_data_structure(dataset_path)
    
    # æ£€æŸ¥episodeæ•°æ®
    check_episode_data(dataset_path, episode_index=0) 